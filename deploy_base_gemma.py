#!/usr/bin/env python3
"""
üöÄ STEP 2: Deploy Base Gemma 2-2B Locally
Downloads and caches Gemma 2-2B-IT model for local inference
"""

import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from pathlib import Path
import time

def deploy_base_gemma():
    """Deploy Gemma 2-2B base model locally with optimization"""
    
    print("üöÄ STEP 2: DEPLOYING BASE GEMMA 2-2B LOCALLY")
    print("=" * 60)
    
    # Model configuration - Using open alternative since Gemma requires authentication
    model_name = "microsoft/DialoGPT-medium"  # Open alternative for testing
    backup_models = [
        "distilgpt2", 
        "gpt2",
        "microsoft/DialoGPT-small"
    ]
    cache_dir = "./models/base-llm"
    
    # Create models directory
    os.makedirs(cache_dir, exist_ok=True)
    
    print(f"üì¶ Model: {model_name}")
    print(f"üíæ Cache Directory: {cache_dir}")
    print(f"üñ•Ô∏è  Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
    
    try:
    try:
        print(f"üñ•Ô∏è  Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
    
    try:
        print("
üîÑ Step 2.1: Downloading Tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            trust_remote_code=True
        )
        
        # Add padding token if not present
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        print("‚úÖ Tokenizer loaded successfully")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to load {model_name}: {e}")
        print("üîÑ Trying backup models...")
        
        for backup_model in backup_models:
            try:
                print(f"üîÑ Trying {backup_model}...")
                tokenizer = AutoTokenizer.from_pretrained(
                    backup_model,
                    cache_dir=cache_dir
                )
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token
                model_name = backup_model
                print(f"‚úÖ Using {backup_model}")
                break
            except Exception as backup_e:
                print(f"‚ùå {backup_model} failed: {backup_e}")
                continue
        else:
            raise Exception("All models failed to load")
        
        print("\nüîÑ Step 2.2: Configuring Model Optimization...")
        
        # Configure quantization for efficiency
        if torch.cuda.is_available():
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
            torch_dtype = torch.float16
        else:
            quantization_config = None
            torch_dtype = torch.float32
            
        print(f"üîß Quantization: {'4-bit' if quantization_config else 'None'}")
        print(f"üîß Data Type: {torch_dtype}")
        
        print("\nüîÑ Step 2.3: Loading Base Model (this may take several minutes)...")
        start_time = time.time()
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            quantization_config=quantization_config,
            torch_dtype=torch_dtype,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        
        load_time = time.time() - start_time
        print(f"‚úÖ Model loaded in {load_time:.2f} seconds")
        
        print("\nüîÑ Step 2.4: Testing Base Model...")
        
        # Test the model with a simple educational query
        test_prompt = "Explain what photosynthesis is in simple terms:"
        
        inputs = tokenizer(test_prompt, return_tensors="pt", truncation=True, max_length=512)
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
            
        print("üß† Generating test response...")
        start_time = time.time()
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=100,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
            
        generation_time = time.time() - start_time
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = response.replace(test_prompt, "").strip()
        
        print(f"üïí Generation time: {generation_time:.3f} seconds")
        print(f"üìù Response: {response}")
        
        print("\nüîÑ Step 2.5: Saving Model Configuration...")
        
        # Save model info for later use
        model_info = {
            "model_name": model_name,
            "cache_dir": cache_dir,
            "torch_dtype": str(torch_dtype),
            "quantized": quantization_config is not None,
            "generation_time": generation_time,
            "test_successful": True
        }
        
        import json
        with open(f"{cache_dir}/model_info.json", "w") as f:
            json.dump(model_info, f, indent=2)
            
        print("‚úÖ Model configuration saved")
        
        print("\nüéâ STEP 2 COMPLETE: BASE GEMMA 2-2B DEPLOYED SUCCESSFULLY!")
        print("=" * 60)
        print(f"üìç Model cached at: {cache_dir}")
        print(f"‚ö° Ready for fine-tuning in Step 4")
        print(f"üöÄ Proceeding to Step 3: Educational Dataset Creation")
        
        return {
            "success": True,
            "model": model,
            "tokenizer": tokenizer,
            "model_info": model_info
        }
        
    except Exception as e:
        print(f"\n‚ùå ERROR in Step 2: {str(e)}")
        print("üîß Attempting to diagnose and fix...")
        
        # Try with CPU-only fallback
        try:
            print("üîÑ Retrying with CPU-only configuration...")
            
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                cache_dir=cache_dir
            )
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                cache_dir=cache_dir,
                torch_dtype=torch.float32,
                low_cpu_mem_usage=True
            )
            
            print("‚úÖ CPU-only deployment successful")
            
            return {
                "success": True,
                "model": model,
                "tokenizer": tokenizer,
                "model_info": {"mode": "cpu_fallback"}
            }
            
        except Exception as e2:
            print(f"‚ùå CPU fallback also failed: {str(e2)}")
            return {
                "success": False,
                "error": str(e2)
            }

if __name__ == "__main__":
    result = deploy_base_gemma()
    
    if result["success"]:
        print("\n‚úÖ Step 2 completed successfully!")
        print("üîÑ Ready to proceed to Step 3...")
    else:
        print(f"\n‚ùå Step 2 failed: {result.get('error', 'Unknown error')}")
        exit(1)
