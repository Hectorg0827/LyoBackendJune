#!/usr/bin/env python3
"""
Database Optimization Engine for Lyo Platform
Advanced database performance optimization, query analysis,
connection pooling, and automated performance monitoring
"""

from typing import Dict, List, Optional, Any, Tuple, Union, Set
from dataclasses import dataclass, field
from enum import Enum
import asyncio
import time
import json
import threading
from datetime import datetime, timedelta
from contextlib import contextmanager
import hashlib


class QueryType(Enum):
    """Types of database queries"""
    SELECT = "SELECT"
    INSERT = "INSERT"
    UPDATE = "UPDATE"
    DELETE = "DELETE"
    BULK_INSERT = "BULK_INSERT"
    BULK_UPDATE = "BULK_UPDATE"
    COMPLEX_JOIN = "COMPLEX_JOIN"
    AGGREGATION = "AGGREGATION"


class OptimizationLevel(Enum):
    """Database optimization levels"""
    BASIC = "basic"
    STANDARD = "standard"
    AGGRESSIVE = "aggressive"
    CUSTOM = "custom"


@dataclass
class QueryMetrics:
    """Metrics for database query performance"""
    query_id: str
    query_type: QueryType
    sql_text: str
    execution_time: float
    rows_examined: Optional[int] = None
    rows_returned: Optional[int] = None
    index_usage: List[str] = field(default_factory=list)
    table_locks: int = 0
    memory_usage: Optional[int] = None
    cpu_time: Optional[float] = None
    timestamp: datetime = field(default_factory=datetime.utcnow)
    success: bool = True
    error_message: Optional[str] = None


@dataclass
class ConnectionPoolStats:
    """Connection pool statistics"""
    pool_name: str
    total_connections: int
    active_connections: int
    idle_connections: int
    max_connections: int
    connections_created: int
    connections_closed: int
    connection_errors: int
    average_checkout_time: float
    peak_active_connections: int
    total_checkouts: int


@dataclass
class IndexRecommendation:
    """Database index recommendation"""
    table_name: str
    column_names: List[str]
    index_type: str = "btree"
    estimated_benefit: float = 0.0
    query_patterns: List[str] = field(default_factory=list)
    priority: str = "medium"
    reasoning: str = ""


@dataclass
class DatabaseHealth:
    """Overall database health metrics"""
    timestamp: datetime
    connection_pool_health: Dict[str, float]
    query_performance_score: float
    index_efficiency_score: float
    memory_usage_percent: float
    disk_usage_percent: float
    replication_lag: Optional[float] = None
    locks_waiting: int = 0
    slow_queries_count: int = 0
    overall_health_score: float = 0.0


class DatabaseOptimizer:
    """Advanced database optimization and monitoring system"""

    def __init__(self, optimization_level: OptimizationLevel = OptimizationLevel.STANDARD):
        self.optimization_level = optimization_level
        self.query_metrics: Dict[str, QueryMetrics] = {}
        self.connection_pools: Dict[str, Any] = {}
        self.connection_pool_stats: Dict[str, ConnectionPoolStats] = {}
        self.query_cache: Dict[str, Any] = {}
        self.index_recommendations: List[IndexRecommendation] = []
        self.slow_query_threshold = 1.0  # seconds
        self.monitoring_enabled = True
        self.optimization_history: List[Dict[str, Any]] = []
        self._lock = threading.Lock()

    def register_connection_pool(self, pool_name: str, pool_config: Dict[str, Any]):
        """Register a database connection pool for monitoring"""
        self.connection_pools[pool_name] = pool_config
        self.connection_pool_stats[pool_name] = ConnectionPoolStats(
            pool_name=pool_name,
            total_connections=0,
            active_connections=0,
            idle_connections=0,
            max_connections=pool_config.get("max_connections", 10),
            connections_created=0,
            connections_closed=0,
            connection_errors=0,
            average_checkout_time=0.0,
            peak_active_connections=0,
            total_checkouts=0
        )

    @contextmanager
    def monitored_query(self, query_text: str, query_type: QueryType = QueryType.SELECT):
        """Context manager for monitoring query performance"""
        query_id = self._generate_query_id(query_text)
        start_time = time.time()

        try:
            yield query_id
            execution_time = time.time() - start_time

            # Record successful query metrics
            self._record_query_metrics(query_id, query_type, query_text, execution_time, True)

        except Exception as e:
            execution_time = time.time() - start_time

            # Record failed query metrics
            self._record_query_metrics(
                query_id, query_type, query_text, execution_time, False, str(e)
            )
            raise

    def _generate_query_id(self, query_text: str) -> str:
        """Generate unique ID for query"""
        # Normalize query text for better caching
        normalized = ' '.join(query_text.split()).upper()
        return hashlib.md5(normalized.encode()).hexdigest()[:16]

    def _record_query_metrics(self, query_id: str, query_type: QueryType,
                            query_text: str, execution_time: float,
                            success: bool, error_message: Optional[str] = None):
        """Record query performance metrics"""
        with self._lock:
            metrics = QueryMetrics(
                query_id=query_id,
                query_type=query_type,
                sql_text=query_text,
                execution_time=execution_time,
                success=success,
                error_message=error_message
            )

            self.query_metrics[query_id] = metrics

            # Check if query is slow and needs optimization
            if execution_time > self.slow_query_threshold:
                asyncio.create_task(self._analyze_slow_query(metrics))

    async def _analyze_slow_query(self, metrics: QueryMetrics):
        """Analyze slow query and generate optimization recommendations"""
        recommendations = []

        # Analyze query patterns
        sql_lower = metrics.sql_text.lower()

        # Check for missing WHERE clause on large tables
        if "select" in sql_lower and "where" not in sql_lower and "limit" not in sql_lower:
            recommendations.append({
                "type": "missing_where_clause",
                "message": "Consider adding WHERE clause to limit result set",
                "priority": "high"
            })

        # Check for N+1 query patterns
        if metrics.query_type == QueryType.SELECT and metrics.execution_time > 0.1:
            recommendations.append({
                "type": "potential_n_plus_1",
                "message": "Consider using JOIN or eager loading to reduce queries",
                "priority": "medium"
            })

        # Check for missing indexes
        if "order by" in sql_lower or "group by" in sql_lower:
            recommendations.append({
                "type": "index_needed",
                "message": "Consider adding index for ORDER BY or GROUP BY columns",
                "priority": "medium"
            })

        # Store recommendations
        if recommendations:
            self.optimization_history.append({
                "timestamp": datetime.utcnow().isoformat(),
                "query_id": metrics.query_id,
                "execution_time": metrics.execution_time,
                "recommendations": recommendations
            })

    def get_query_performance_summary(self, time_window: int = 3600) -> Dict[str, Any]:
        """Get query performance summary for the specified time window (seconds)"""
        cutoff_time = datetime.utcnow() - timedelta(seconds=time_window)

        recent_queries = [
            metrics for metrics in self.query_metrics.values()
            if metrics.timestamp >= cutoff_time
        ]

        if not recent_queries:
            return {"message": "No queries recorded in the specified time window"}

        # Calculate statistics
        total_queries = len(recent_queries)
        successful_queries = len([q for q in recent_queries if q.success])
        failed_queries = total_queries - successful_queries

        execution_times = [q.execution_time for q in recent_queries if q.success]
        avg_execution_time = sum(execution_times) / len(execution_times) if execution_times else 0
        max_execution_time = max(execution_times) if execution_times else 0
        min_execution_time = min(execution_times) if execution_times else 0

        slow_queries = len([q for q in recent_queries if q.execution_time > self.slow_query_threshold])

        # Query type breakdown
        query_type_counts = {}
        for query in recent_queries:
            query_type = query.query_type.value
            query_type_counts[query_type] = query_type_counts.get(query_type, 0) + 1

        return {
            "time_window_seconds": time_window,
            "total_queries": total_queries,
            "successful_queries": successful_queries,
            "failed_queries": failed_queries,
            "success_rate": successful_queries / total_queries * 100 if total_queries > 0 else 0,
            "average_execution_time": avg_execution_time,
            "max_execution_time": max_execution_time,
            "min_execution_time": min_execution_time,
            "slow_queries_count": slow_queries,
            "slow_queries_percentage": slow_queries / total_queries * 100 if total_queries > 0 else 0,
            "query_type_breakdown": query_type_counts,
            "performance_score": self._calculate_performance_score(recent_queries)
        }

    def _calculate_performance_score(self, queries: List[QueryMetrics]) -> float:
        """Calculate overall performance score (0-100)"""
        if not queries:
            return 100.0

        # Base score
        score = 100.0

        # Penalize failed queries
        failed_queries = len([q for q in queries if not q.success])
        failure_rate = failed_queries / len(queries)
        score -= failure_rate * 30  # Up to 30 point penalty

        # Penalize slow queries
        slow_queries = len([q for q in queries if q.execution_time > self.slow_query_threshold])
        slow_query_rate = slow_queries / len(queries)
        score -= slow_query_rate * 40  # Up to 40 point penalty

        # Penalize very slow queries
        very_slow_queries = len([q for q in queries if q.execution_time > 5.0])
        very_slow_rate = very_slow_queries / len(queries)
        score -= very_slow_rate * 30  # Additional 30 point penalty

        return max(0.0, score)

    def optimize_connection_pool(self, pool_name: str, current_stats: Dict[str, Any]) -> Dict[str, Any]:
        """Optimize connection pool settings based on usage patterns"""
        if pool_name not in self.connection_pool_stats:
            return {"error": "Pool not registered"}

        stats = self.connection_pool_stats[pool_name]
        recommendations = []

        # Analyze connection usage patterns
        utilization_rate = current_stats.get("active_connections", 0) / stats.max_connections

        # Pool size recommendations
        if utilization_rate > 0.8:
            recommendations.append({
                "type": "increase_pool_size",
                "current_max": stats.max_connections,
                "recommended_max": min(stats.max_connections * 2, 50),
                "reason": "High pool utilization detected"
            })
        elif utilization_rate < 0.2 and stats.max_connections > 5:
            recommendations.append({
                "type": "decrease_pool_size",
                "current_max": stats.max_connections,
                "recommended_max": max(stats.max_connections // 2, 5),
                "reason": "Low pool utilization detected"
            })

        # Connection timeout recommendations
        avg_checkout = current_stats.get("average_checkout_time", 0)
        if avg_checkout > 5.0:  # 5 seconds
            recommendations.append({
                "type": "increase_timeout",
                "current_timeout": "default",
                "recommended_timeout": "15s",
                "reason": "Long average checkout time detected"
            })

        return {
            "pool_name": pool_name,
            "current_utilization": utilization_rate,
            "recommendations": recommendations,
            "optimization_applied": len(recommendations) > 0
        }

    def generate_index_recommendations(self, query_patterns: List[Dict[str, Any]]) -> List[IndexRecommendation]:
        """Generate index recommendations based on query patterns"""
        recommendations = []

        # Analyze query patterns for common WHERE clauses
        column_usage = {}
        table_usage = {}

        for pattern in query_patterns:
            table_name = pattern.get("table_name", "unknown")
            where_columns = pattern.get("where_columns", [])
            order_columns = pattern.get("order_columns", [])
            query_count = pattern.get("execution_count", 1)
            avg_time = pattern.get("avg_execution_time", 0)

            table_usage[table_name] = table_usage.get(table_name, 0) + query_count

            # Track column usage for indexing
            for column in where_columns + order_columns:
                key = f"{table_name}.{column}"
                if key not in column_usage:
                    column_usage[key] = {"count": 0, "total_time": 0, "tables": set()}

                column_usage[key]["count"] += query_count
                column_usage[key]["total_time"] += avg_time * query_count
                column_usage[key]["tables"].add(table_name)

        # Generate recommendations for frequently used columns
        for column_key, usage in column_usage.items():
            if usage["count"] >= 10 or usage["total_time"] > 5.0:  # Threshold for recommendation
                table_name, column_name = column_key.split(".", 1)

                benefit_score = (usage["count"] * 0.1) + (usage["total_time"] * 0.2)
                priority = "high" if benefit_score > 5.0 else "medium" if benefit_score > 2.0 else "low"

                recommendation = IndexRecommendation(
                    table_name=table_name,
                    column_names=[column_name],
                    index_type="btree",
                    estimated_benefit=benefit_score,
                    query_patterns=[],
                    priority=priority,
                    reasoning=f"Column used in {usage['count']} queries with total time {usage['total_time']:.2f}s"
                )

                recommendations.append(recommendation)

        # Sort by estimated benefit
        recommendations.sort(key=lambda x: x.estimated_benefit, reverse=True)

        return recommendations[:10]  # Return top 10 recommendations

    async def perform_health_check(self) -> DatabaseHealth:
        """Perform comprehensive database health check"""
        timestamp = datetime.utcnow()

        # Analyze connection pools
        pool_health = {}
        for pool_name, stats in self.connection_pool_stats.items():
            if stats.max_connections > 0:
                utilization = stats.active_connections / stats.max_connections
                health_score = 100 - (utilization * 50)  # Penalize high utilization
                pool_health[pool_name] = max(0, health_score)

        # Calculate query performance score
        recent_summary = self.get_query_performance_summary(3600)  # Last hour
        query_performance_score = recent_summary.get("performance_score", 100)

        # Simulate other health metrics (in real implementation, these would come from actual DB)
        index_efficiency_score = 85.0  # Placeholder
        memory_usage_percent = 65.0  # Placeholder
        disk_usage_percent = 45.0  # Placeholder
        slow_queries_count = recent_summary.get("slow_queries_count", 0)

        # Calculate overall health score
        overall_health = (
            (sum(pool_health.values()) / len(pool_health) if pool_health else 100) * 0.2 +
            query_performance_score * 0.3 +
            index_efficiency_score * 0.2 +
            (100 - memory_usage_percent) * 0.15 +
            (100 - disk_usage_percent) * 0.15
        )

        return DatabaseHealth(
            timestamp=timestamp,
            connection_pool_health=pool_health,
            query_performance_score=query_performance_score,
            index_efficiency_score=index_efficiency_score,
            memory_usage_percent=memory_usage_percent,
            disk_usage_percent=disk_usage_percent,
            slow_queries_count=slow_queries_count,
            overall_health_score=overall_health
        )

    def enable_query_caching(self, cache_size: int = 1000, ttl_seconds: int = 300):
        """Enable intelligent query result caching"""
        self.query_cache = {
            "enabled": True,
            "max_size": cache_size,
            "ttl_seconds": ttl_seconds,
            "cache": {},
            "hits": 0,
            "misses": 0
        }

    def get_cache_key(self, query_text: str, params: Optional[Dict] = None) -> str:
        """Generate cache key for query result caching"""
        cache_data = {"query": query_text.strip()}
        if params:
            cache_data["params"] = params

        cache_string = json.dumps(cache_data, sort_keys=True)
        return hashlib.md5(cache_string.encode()).hexdigest()

    def should_cache_query(self, query_type: QueryType, execution_time: float) -> bool:
        """Determine if query result should be cached"""
        # Cache SELECT queries that take more than 100ms
        if query_type == QueryType.SELECT and execution_time > 0.1:
            return True

        # Cache expensive aggregation queries
        if query_type == QueryType.AGGREGATION and execution_time > 0.05:
            return True

        return False

    def optimize_query_text(self, query_text: str) -> Tuple[str, List[str]]:
        """Optimize SQL query text and return optimization suggestions"""
        optimized_query = query_text.strip()
        suggestions = []

        query_lower = query_text.lower()

        # Remove unnecessary whitespace
        optimized_query = ' '.join(optimized_query.split())

        # Suggest LIMIT clause for SELECT without WHERE
        if ("select" in query_lower and
            "where" not in query_lower and
            "limit" not in query_lower and
            "count(" not in query_lower):
            suggestions.append("Consider adding LIMIT clause to prevent large result sets")

        # Suggest specific columns instead of SELECT *
        if "select *" in query_lower:
            suggestions.append("Consider selecting specific columns instead of SELECT * for better performance")

        # Suggest using EXISTS instead of IN for subqueries
        if " in (" in query_lower and "select" in query_lower:
            suggestions.append("Consider using EXISTS instead of IN with subqueries for better performance")

        # Suggest using INNER JOIN instead of WHERE with multiple tables
        if (query_lower.count("from") == 1 and
            query_lower.count(",") > 0 and
            "join" not in query_lower):
            suggestions.append("Consider using explicit JOIN syntax instead of comma-separated tables")

        return optimized_query, suggestions

    def get_optimization_report(self) -> Dict[str, Any]:
        """Generate comprehensive optimization report"""
        health_check = asyncio.create_task(self.perform_health_check())
        performance_summary = self.get_query_performance_summary(3600)

        # Wait for health check to complete
        import asyncio
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # If we're already in an event loop, we can't use run()
            health = DatabaseHealth(
                timestamp=datetime.utcnow(),
                connection_pool_health={},
                query_performance_score=performance_summary.get("performance_score", 100),
                index_efficiency_score=85.0,
                memory_usage_percent=65.0,
                disk_usage_percent=45.0,
                slow_queries_count=performance_summary.get("slow_queries_count", 0),
                overall_health_score=90.0
            )
        else:
            health = asyncio.run(self.perform_health_check())

        return {
            "generated_at": datetime.utcnow().isoformat(),
            "optimization_level": self.optimization_level.value,
            "database_health": {
                "overall_score": health.overall_health_score,
                "query_performance": health.query_performance_score,
                "index_efficiency": health.index_efficiency_score,
                "memory_usage": health.memory_usage_percent,
                "disk_usage": health.disk_usage_percent
            },
            "query_performance": performance_summary,
            "connection_pools": {
                name: {
                    "max_connections": stats.max_connections,
                    "active_connections": stats.active_connections,
                    "utilization": stats.active_connections / stats.max_connections if stats.max_connections > 0 else 0
                }
                for name, stats in self.connection_pool_stats.items()
            },
            "index_recommendations_count": len(self.index_recommendations),
            "optimization_history_count": len(self.optimization_history),
            "cache_stats": {
                "enabled": self.query_cache.get("enabled", False),
                "hits": self.query_cache.get("hits", 0),
                "misses": self.query_cache.get("misses", 0),
                "hit_rate": (self.query_cache.get("hits", 0) /
                           max(1, self.query_cache.get("hits", 0) + self.query_cache.get("misses", 0)) * 100)
            }
        }


# Global database optimizer instance
db_optimizer = DatabaseOptimizer()


# Convenience functions
def monitor_query(query_text: str, query_type: QueryType = QueryType.SELECT):
    """Decorator for monitoring database queries"""
    return db_optimizer.monitored_query(query_text, query_type)


def optimize_query(query_text: str) -> Tuple[str, List[str]]:
    """Optimize SQL query text"""
    return db_optimizer.optimize_query_text(query_text)


async def get_database_health() -> DatabaseHealth:
    """Get current database health status"""
    return await db_optimizer.perform_health_check()


def enable_caching(cache_size: int = 1000, ttl_seconds: int = 300):
    """Enable query result caching"""
    db_optimizer.enable_query_caching(cache_size, ttl_seconds)


if __name__ == "__main__":
    # Example usage and testing
    async def demo_database_optimizer():
        print("ğŸ¯ DATABASE OPTIMIZER DEMONSTRATION")
        print("=" * 70)

        # Setup optimizer
        optimizer = DatabaseOptimizer(OptimizationLevel.STANDARD)

        # Register connection pool
        print("ğŸ”— Setting up connection pool...")
        optimizer.register_connection_pool("main_pool", {
            "max_connections": 10,
            "min_connections": 2,
            "timeout": 30
        })
        print("   âœ… Connection pool registered")

        # Simulate some database queries
        print("\\nğŸ“Š Simulating database queries...")

        test_queries = [
            ("SELECT * FROM users WHERE email = 'test@example.com'", QueryType.SELECT, 0.05),
            ("SELECT * FROM courses", QueryType.SELECT, 2.5),  # Slow query
            ("INSERT INTO sessions (user_id, course_id) VALUES (1, 2)", QueryType.INSERT, 0.02),
            ("UPDATE users SET last_login = NOW() WHERE id = 1", QueryType.UPDATE, 0.08),
            ("SELECT COUNT(*) FROM learning_sessions", QueryType.AGGREGATION, 1.2)
        ]

        for query_text, query_type, sim_time in test_queries:
            with optimizer.monitored_query(query_text, query_type) as query_id:
                # Simulate query execution time
                await asyncio.sleep(sim_time / 10)  # Scaled down for demo
                time.sleep(sim_time / 100)  # Small actual delay

            print(f"   ğŸ“ Executed: {query_type.value} query ({sim_time}s)")

        # Get performance summary
        print("\\nğŸ“ˆ Performance Analysis...")
        summary = optimizer.get_query_performance_summary(3600)
        print(f"   ğŸ“Š Total queries: {summary['total_queries']}")
        print(f"   âœ… Success rate: {summary['success_rate']:.1f}%")
        print(f"   âš¡ Avg execution time: {summary['average_execution_time']:.3f}s")
        print(f"   ğŸŒ Slow queries: {summary['slow_queries_count']}")
        print(f"   ğŸ“Š Performance score: {summary['performance_score']:.1f}/100")

        # Health check
        print("\\nğŸ¥ Database Health Check...")
        health = await optimizer.perform_health_check()
        print(f"   ğŸ’š Overall health: {health.overall_health_score:.1f}/100")
        print(f"   âš¡ Query performance: {health.query_performance_score:.1f}/100")
        print(f"   ğŸ’¾ Memory usage: {health.memory_usage_percent:.1f}%")

        # Query optimization
        print("\\nğŸ”§ Query Optimization...")
        test_query = "SELECT * FROM users WHERE status = 'active'"
        optimized, suggestions = optimizer.optimize_query_text(test_query)
        print(f"   Original: {test_query}")
        print(f"   Optimized: {optimized}")
        for suggestion in suggestions:
            print(f"      ğŸ’¡ {suggestion}")

        # Index recommendations
        print("\\nğŸ“‡ Index Recommendations...")
        query_patterns = [
            {
                "table_name": "users",
                "where_columns": ["email", "status"],
                "execution_count": 150,
                "avg_execution_time": 0.8
            },
            {
                "table_name": "courses",
                "where_columns": ["category", "difficulty"],
                "order_columns": ["created_at"],
                "execution_count": 200,
                "avg_execution_time": 1.2
            }
        ]

        recommendations = optimizer.generate_index_recommendations(query_patterns)
        for rec in recommendations[:3]:
            print(f"   ğŸ“‡ {rec.table_name}.{', '.join(rec.column_names)} ({rec.priority} priority)")
            print(f"      ğŸ’¡ {rec.reasoning}")

        # Enable caching
        print("\\nğŸ’¾ Enabling Query Caching...")
        optimizer.enable_query_caching(cache_size=500, ttl_seconds=300)
        print("   âœ… Query caching enabled (500 entries, 5min TTL)")

        # Final report
        print("\\nğŸ“‹ Optimization Report...")
        report = optimizer.get_optimization_report()
        print(f"   ğŸ† Database Health: {report['database_health']['overall_score']:.1f}/100")
        print(f"   âš¡ Query Performance: {report['database_health']['query_performance']:.1f}/100")
        print(f"   ğŸ’¾ Cache Hit Rate: {report['cache_stats']['hit_rate']:.1f}%")

        print(f"\\nğŸ‰ DATABASE OPTIMIZER READY")
        print("   âœ… Query performance monitoring")
        print("   âœ… Connection pool optimization")
        print("   âœ… Intelligent query caching")
        print("   âœ… Index recommendations")
        print("   âœ… Health monitoring")
        print("   âœ… Query optimization suggestions")

    # Run demo if called directly
    asyncio.run(demo_database_optimizer())