 [pytest]
 addopts = -q
 testpaths = tests
 python_files = test_*.py
 filterwarnings = ignore::DeprecationWarning


# Environment management
@pytest.fixture(scope="session", autouse=True)
def setup_test_environment():
    """Setup test environment variables."""
    # Store original environment
    original_env = {}

    test_env_vars = {
        "ENVIRONMENT": "testing",
        "DEBUG": "False",
        "TESTING": "True",
        "SECRET_KEY": "test-secret-key-for-testing-only-" + "x" * 32,
        "DATABASE_URL": os.getenv("TEST_DATABASE_URL", "sqlite+aiosqlite:///:memory:"),
        "REDIS_URL": "redis://localhost:6379/1",
        "GOOGLE_API_KEY": "test-google-api-key"
    }

    # Set test environment variables
    for key, value in test_env_vars.items():
        original_env[key] = os.environ.get(key)
        os.environ[key] = value

    yield

    # Restore original environment
    for key, value in original_env.items():
        if value is None:
            os.environ.pop(key, None)
        else:
            os.environ[key] = value


# Performance reporting
@pytest.fixture(scope="session")
def performance_report():
    """Collect performance test results."""
    report_data = {
        "tests": [],
        "summary": {
            "total_tests": 0,
            "passed": 0,
            "failed": 0,
            "avg_response_time": 0.0,
            "max_response_time": 0.0,
            "min_response_time": float('inf')
        }
    }

    yield report_data

    # Generate report if requested
    if os.environ.get("PYTEST_REPORT_PERFORMANCE"):
        generate_performance_report(report_data)


def generate_performance_report(report_data):
    """Generate performance test report."""
    if not report_data["tests"]:
        return

    print("\n" + "="*60)
    print("PERFORMANCE TEST REPORT")
    print("="*60)

    print(f"Total Tests: {report_data['summary']['total_tests']}")
    print(f"Passed: {report_data['summary']['passed']}")
    print(f"Failed: {report_data['summary']['failed']}")

    if report_data["tests"]:
        print(f"Average Response Time: {report_data['summary']['avg_response_time']:.3f}s")
        print(f"Max Response Time: {report_data['summary']['max_response_time']:.3f}s")
        print(f"Min Response Time: {report_data['summary']['min_response_time']:.3f}s")

    print("\nDetailed Results:")
    print("-" * 40)

    for test in report_data["tests"]:
        status = "✓" if test["passed"] else "✗"
        print(f"{status} {test['name']}: {test['response_time']:.3f}s")

    print("="*60)


# Test result hooks
@pytest.hookimpl(tryfirst=True)
def pytest_runtest_makereport(item, call):
    """Hook to capture test results."""
    if call.when == "call" and hasattr(item, "performance_report"):
        # This would be used to collect performance data
        # Implementation depends on how performance data is collected
        pass


# Custom test selection
def pytest_ignore_collect(path, config):
    """Ignore certain test files based on conditions."""
    # Ignore performance tests if not explicitly enabled
    if "performance" in str(path) and not config.getoption("--enable-slow-tests"):
        # Only ignore if it's a performance test and slow tests are disabled
        if "slow" in str(path) or "performance" in str(path):
            return True

    return False


# Test configuration validation
def pytest_configure_node(node):
    """Configure individual test nodes."""
    # Add custom metadata to tests
    if hasattr(node, "keywords"):
        if "performance" in node.keywords:
            node.add_marker(pytest.mark.performance)
        if "slow" in node.keywords:
            node.add_marker(pytest.mark.slow)


# Test data fixtures
@pytest.fixture
def sample_user_data():
    """Provide sample user data for tests."""
    return {
        "email": "test@example.com",
        "username": "testuser",
        "password": "testpass123",
        "full_name": "Test User",
        "bio": "Test bio",
        "is_active": True,
        "is_verified": True
    }


@pytest.fixture
def sample_course_data():
    """Provide sample course data for tests."""
    return {
        "title": "Test Course",
        "description": "A test course",
        "instructor": "Test Instructor",
        "duration": 60,
        "difficulty": "beginner",
        "category": "programming",
        "tags": ["test", "programming"],
        "thumbnail_url": "https://example.com/thumbnail.jpg"
    }


@pytest.fixture
def sample_post_data():
    """Provide sample post data for tests."""
    return {
        "content": "This is a test post",
        "image_urls": ["https://example.com/image1.jpg"],
        "video_url": None,
        "is_public": True,
        "tags": ["test", "sample"]
    }


# Database fixtures
@pytest.fixture(scope="function")
def db_cleanup(db_session):
    """Clean up database after each test."""
    yield
    # Add cleanup logic here if needed
    # For example, truncate tables or delete test data


# Mock fixtures
@pytest.fixture
def mock_external_service():
    """Mock external service calls."""
    # This can be used to mock external API calls, email services, etc.
    class MockExternalService:
        def __init__(self):
            self.calls = []

        def send_email(self, to, subject, body):
            self.calls.append({
                "method": "send_email",
                "to": to,
                "subject": subject,
                "body": body
            })
            return True

        def upload_file(self, file, destination):
            self.calls.append({
                "method": "upload_file",
                "file": file,
                "destination": destination
            })
            return f"https://example.com/{destination}"

    return MockExternalService()


# Test utilities
def assert_api_response(response, expected_status=200, expected_keys=None):
    """Assert API response has expected status and keys."""
    assert response.status_code == expected_status

    if expected_keys:
        data = response.json()
        for key in expected_keys:
            assert key in data, f"Expected key '{key}' not found in response"


def assert_paginated_response(response, min_items=0, max_items=None):
    """Assert response is properly paginated."""
    assert response.status_code == 200
    data = response.json()

    assert "items" in data
    assert "total" in data
    assert "page" in data
    assert "page_size" in data

    if min_items > 0:
        assert len(data["items"]) >= min_items

    if max_items is not None:
        assert len(data["items"]) <= max_items


def assert_error_response(response, expected_status=400, expected_error_code=None):
    """Assert response is an error with expected status and code."""
    assert response.status_code == expected_status
    data = response.json()
    assert "error" in data

    if expected_error_code:
        assert data["error"]["code"] == expected_error_code


# Performance test utilities
def measure_execution_time(func):
    """Decorator to measure function execution time."""
    import time
    from functools import wraps

    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()

        execution_time = end_time - start_time
        print(f"{func.__name__} executed in {execution_time:.3f} seconds")

        return result

    return wrapper


def benchmark_function(func, iterations=100):
    """Benchmark a function over multiple iterations."""
    import time

    times = []

    for _ in range(iterations):
        start_time = time.time()
        func()
        end_time = time.time()
        times.append(end_time - start_time)

    avg_time = sum(times) / len(times)
    min_time = min(times)
    max_time = max(times)

    return {
        "iterations": iterations,
        "avg_time": avg_time,
        "min_time": min_time,
        "max_time": max_time,
        "total_time": sum(times)
    }
